{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package snowball_data is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('snowball_data')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list=set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_lst(fichier):\n",
    "    with open(fichier,'r') as file:\n",
    "        content=file.read()\n",
    "    content=content.lower()\n",
    "    tokens=re.split(r\"\\W\",content)\n",
    "    mots=[t for t in tokens if not(re.match(r\"([0-9]+-?)+\",t)) and t not in stop_list]\n",
    "    stemmer=SnowballStemmer(\"french\")\n",
    "    stems=[stemmer.stem(mot) for mot in mots]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF(fichier):\n",
    "    stems=stem_lst(fichier)\n",
    "    dictionnaire={}\n",
    "    for item in set(stems):\n",
    "        dictionnaire[item]=stems.count(item)\n",
    "    return dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionnaire_global():\n",
    "        liste_doc=['C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\SEMESTRE_2\\\\indexation\\\\corpus\\\\doc'+str(i)+'.txt' for i in range(1,11)]\n",
    "        dic1={}\n",
    "        for fich in liste_doc:\n",
    "            dic1[fich]=TF(fich)\n",
    "        return dic1\n",
    "\n",
    "def IDF(terme):\n",
    "    N=0\n",
    "    for i in ['C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\SEMESTRE_2\\\\indexation\\\\corpus\\\\doc'+str(i)+'.txt' for i in range(1,11)]:\n",
    "        stems=stem_lst(i)\n",
    "        if terme in stems:\n",
    "            N+=1\n",
    "            break\n",
    "    return 1/N\n",
    "def poids(stem,doc):\n",
    "    return TF(doc)[stem]*IDF(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---doc1---7.0\n",
      "or---doc1---1.0\n",
      "a---doc1---2.0\n",
      "peu---doc1---1.0\n",
      "dor---doc1---1.0\n",
      "bout---doc1---1.0\n",
      "échos---doc1---1.0\n",
      "trop---doc1---1.0\n",
      "fer---doc1---1.0\n",
      "juillet---doc1---1.0\n",
      "papi---doc1---2.0\n",
      "mer---doc1---1.0\n",
      "rescap---doc1---1.0\n",
      "terr---doc1---2.0\n",
      "ru---doc1---1.0\n",
      "hui---doc1---1.0\n",
      "grand---doc1---2.0\n",
      "apatrid---doc1---1.0\n",
      "esclav---doc1---2.0\n",
      "couteau---doc1---2.0\n",
      "autrefois---doc1---1.0\n",
      "adolescent---doc1---1.0\n",
      "embauch---doc1---1.0\n",
      "rosi---doc1---1.0\n",
      "dépatri---doc1---1.0\n",
      "tunisien---doc1---1.0\n",
      "boît---doc1---1.0\n",
      "brûleur---doc1---1.0\n",
      "bien---doc1---1.0\n",
      "bastill---doc1---1.0\n",
      "souti---doc1---1.0\n",
      "soleil---doc1---1.0\n",
      "enfant---doc1---3.0\n",
      "kabyl---doc1---1.0\n",
      "ébouillanteur---doc1---1.0\n",
      "retourn---doc1---1.0\n",
      "jol---doc1---1.0\n",
      "étranger---doc1---1.0\n",
      "cigar---doc1---1.0\n",
      "désoeuvr---doc1---1.0\n",
      "indochinois---doc1---1.0\n",
      "allé---doc1---1.0\n",
      "capital---doc1---1.0\n",
      "soir---doc1---1.0\n",
      "bêt---doc1---1.0\n",
      "pris---doc1---1.0\n",
      "barcelon---doc1---1.0\n",
      "polack---doc1---1.0\n",
      "libert---doc1---1.0\n",
      "local---doc1---1.0\n",
      "visag---doc1---1.0\n",
      "vit---doc1---1.0\n",
      "ital---doc1---1.0\n",
      "navarr---doc1---1.0\n",
      "vend---doc1---1.0\n",
      "tôt---doc1---1.0\n",
      "évoqu---doc1---1.0\n",
      "retour---doc1---1.0\n",
      "défendu---doc1---1.0\n",
      "baign---doc1---1.0\n",
      "vieil---doc1---1.0\n",
      "milieu---doc1---1.0\n",
      "disciplinair---doc1---1.0\n",
      "petit---doc1---3.0\n",
      "villag---doc1---1.0\n",
      "sénégal---doc1---1.0\n",
      "parqu---doc1---1.0\n",
      "monnai---doc1---1.0\n",
      "oiseau---doc1---1.0\n",
      "déport---doc1---1.0\n",
      "baléar---doc1---1.0\n",
      "tous---doc1---2.0\n",
      "quais---doc1---1.0\n",
      "cadenc---doc1---1.0\n",
      "cordonni---doc1---1.0\n",
      "paris---doc1---1.0\n",
      "doux---doc1---1.0\n",
      "chaqu---doc1---1.0\n",
      "débauch---doc1---1.0\n",
      "incendiair---doc1---1.0\n",
      "port---doc1---2.0\n",
      "autr---doc1---1.0\n",
      "ven---doc1---1.0\n",
      "finisterr---doc1---1.0\n",
      "aubervilli---doc1---1.0\n",
      "javel---doc1---1.0\n",
      "grenel---doc1---1.0\n",
      "si---doc1---3.0\n",
      "mour---doc1---1.0\n",
      "dorm---doc1---1.0\n",
      "boumian---doc1---1.0\n",
      "fréjus---doc1---2.0\n",
      "labour---doc1---1.0\n",
      "renvoi---doc1---1.0\n",
      "pay---doc1---2.0\n",
      "cobay---doc1---1.0\n",
      "fait---doc1---1.0\n",
      "cordou---doc1---1.0\n",
      "dos---doc1---1.0\n",
      "fêt---doc1---1.0\n",
      "quatorz---doc1---1.0\n",
      "ouen---doc1---1.0\n",
      "saint---doc1---1.0\n",
      "viv---doc1---1.0\n",
      "pied---doc1---1.0\n",
      "mar---doc1---1.0\n",
      "templ---doc1---1.0\n",
      "vôtr---doc1---1.0\n",
      "franc---doc1---1.0\n",
      "quelqu---doc1---1.0\n",
      "expatri---doc1---1.0\n",
      "fil---doc1---1.0\n",
      "musicien---doc1---1.0\n",
      "bord---doc1---1.0\n",
      "lointain---doc1---1.0\n",
      "vill---doc1---2.0\n",
      "forêt---doc1---1.0\n",
      "innocent---doc1---1.0\n",
      "naturalis---doc1---1.0\n",
      "vi---doc1---1.0\n",
      "beau---doc1---1.0\n",
      "pli---doc1---1.0\n",
      "tiraill---doc1---1.0\n",
      "souven---doc1---1.0\n",
      "bomb---doc1---1.0\n",
      "colon---doc1---1.0\n",
      "franco---doc1---1.0\n",
      "homm---doc1---1.0\n",
      "manoeuvr---doc1---1.0\n",
      "caf---doc1---1.0\n",
      "pêcheur---doc1---1.0\n",
      "mal---doc1---1.0\n",
      "trouv---doc1---1.0\n",
      "aujourd---doc1---1.0\n",
      "mort---doc1---1.0\n",
      "noir---doc1---2.0\n",
      "dragon---doc1---1.0\n",
      "jongleur---doc1---1.0\n",
      "chapel---doc1---1.0\n",
      "rizi---doc1---1.0\n",
      "avoir---doc1---1.0\n",
      "ordur---doc1---1.0\n",
      "étrang---doc1---1.0\n",
      "où---doc1---1.0\n",
      "---doc2---32.0\n",
      "alor---doc2---1.0\n",
      "enfant---doc2---10.0\n",
      "quand---doc2---1.0\n",
      "redeviennent---doc2---2.0\n",
      "a---doc2---2.0\n",
      "cri---doc2---1.0\n",
      "façon---doc2---1.0\n",
      "vont---doc2---3.0\n",
      "port---doc2---1.0\n",
      "plum---doc2---1.0\n",
      "encre---doc2---1.0\n",
      "autr---doc2---1.0\n",
      "entendent---doc2---2.0\n",
      "descend---doc2---1.0\n",
      "surtout---doc2---1.0\n",
      "cach---doc2---1.0\n",
      "ni---doc2---2.0\n",
      "fair---doc2---1.0\n",
      "professeur---doc2---1.0\n",
      "seiz---doc2---7.0\n",
      "trent---doc2---1.0\n",
      "sauv---doc2---1.0\n",
      "camp---doc2---1.0\n",
      "deux---doc2---10.0\n",
      "crai---doc2---1.0\n",
      "maîtr---doc2---2.0\n",
      "voilà---doc2---1.0\n",
      "tout---doc2---1.0\n",
      "tranquill---doc2---1.0\n",
      "chanson---doc2---1.0\n",
      "égal---doc2---1.0\n",
      "pupitr---doc2---2.0\n",
      "entend---doc2---1.0\n",
      "chant---doc2---1.0\n",
      "font---doc2---6.0\n",
      "écoutent---doc2---1.0\n",
      "lyr---doc2---2.0\n",
      "musiqu---doc2---2.0\n",
      "redevient---doc2---3.0\n",
      "huit---doc2---11.0\n",
      "ciel---doc2---1.0\n",
      "mur---doc2---1.0\n",
      "fichent---doc2---1.0\n",
      "class---doc2---1.0\n",
      "écroulent---doc2---1.0\n",
      "répet---doc2---2.0\n",
      "rien---doc2---1.0\n",
      "tour---doc2---2.0\n",
      "eau---doc2---1.0\n",
      "jou---doc2---5.0\n",
      "arbre---doc2---1.0\n",
      "falais---doc2---1.0\n",
      "oiseau---doc2---7.0\n",
      "fin---doc2---1.0\n",
      "pass---doc2---1.0\n",
      "sabl---doc2---1.0\n",
      "pitr---doc2---1.0\n",
      "tous---doc2---3.0\n",
      "voit---doc2---1.0\n",
      "dit---doc2---2.0\n",
      "vitr---doc2---1.0\n",
      "quatr---doc2---11.0\n",
      "appel---doc2---1.0\n",
      "tableau---doc3---1.0\n",
      "nom---doc3---1.0\n",
      "bonheur---doc3---1.0\n",
      "---doc3---1.0\n",
      "enfant---doc3---1.0\n",
      "non---doc3---2.0\n",
      "problem---doc3---1.0\n",
      "fou---doc3---1.0\n",
      "rir---doc3---1.0\n",
      "effac---doc3---1.0\n",
      "coeur---doc3---1.0\n",
      "prodig---doc3---1.0\n",
      "oui---doc3---2.0\n",
      "professeur---doc3---1.0\n",
      "hu---doc3---1.0\n",
      "malgr---doc3---1.0\n",
      "malheur---doc3---1.0\n",
      "aim---doc3---1.0\n",
      "visag---doc3---1.0\n",
      "crai---doc3---1.0\n",
      "chiffr---doc3---1.0\n",
      "maîtr---doc3---1.0\n",
      "tout---doc3---2.0\n",
      "dessin---doc3---1.0\n",
      "prend---doc3---1.0\n",
      "pieg---doc3---1.0\n",
      "debout---doc3---1.0\n",
      "pos---doc3---1.0\n",
      "phras---doc3---1.0\n",
      "dat---doc3---1.0\n",
      "têt---doc3---1.0\n",
      "couleur---doc3---1.0\n",
      "noir---doc3---1.0\n",
      "mot---doc3---1.0\n",
      "sous---doc3---1.0\n",
      "menac---doc3---1.0\n",
      "question---doc3---1.0\n",
      "soudain---doc3---1.0\n",
      "tous---doc3---1.0\n",
      "dit---doc3---4.0\n",
      "---doc4---1.0\n",
      "non---doc4---1.0\n",
      "ceux---doc4---1.0\n",
      "donnent---doc4---1.0\n",
      "aim---doc4---4.0\n",
      "seul---doc4---1.0\n",
      "voit---doc4---1.0\n",
      "regard---doc4---4.0\n",
      "droit---doc4---1.0\n",
      "vi---doc4---1.0\n",
      "plus---doc4---1.0\n",
      "cel---doc4---1.0\n",
      "---doc5---3.0\n",
      "roug---doc5---1.0\n",
      "souvien---doc5---2.0\n",
      "a---doc5---7.0\n",
      "or---doc5---1.0\n",
      "étiquet---doc5---1.0\n",
      "jet---doc5---1.0\n",
      "bout---doc5---1.0\n",
      "souvenir---doc5---3.0\n",
      "hi---doc5---1.0\n",
      "tout---doc5---2.0\n",
      "paternel---doc5---1.0\n",
      "con---doc5---1.0\n",
      "tranquill---doc5---1.0\n",
      "gross---doc5---2.0\n",
      "hindou---doc5---1.0\n",
      "plac---doc5---1.0\n",
      "éleveur---doc5---1.0\n",
      "grand---doc5---6.0\n",
      "tort---doc5---1.0\n",
      "mauv---doc5---3.0\n",
      "sien---doc5---1.0\n",
      "peut---doc5---3.0\n",
      "gris---doc5---1.0\n",
      "val---doc5---1.0\n",
      "lettr---doc5---1.0\n",
      "bien---doc5---1.0\n",
      "haut---doc5---1.0\n",
      "concierg---doc5---1.0\n",
      "dev---doc5---1.0\n",
      "quatr---doc5---1.0\n",
      "air---doc5---2.0\n",
      "sais---doc5---1.0\n",
      "dégringol---doc5---1.0\n",
      "per---doc5---1.0\n",
      "dir---doc5---1.0\n",
      "osi---doc5---1.0\n",
      "plus---doc5---3.0\n",
      "gravat---doc5---1.0\n",
      "corridor---doc5---1.0\n",
      "soir---doc5---1.0\n",
      "futil---doc5---1.0\n",
      "devenu---doc5---1.0\n",
      "pris---doc5---1.0\n",
      "voilà---doc5---1.0\n",
      "chos---doc5---2.0\n",
      "tard---doc5---1.0\n",
      "log---doc5---1.0\n",
      "musiqu---doc5---1.0\n",
      "rentr---doc5---1.0\n",
      "choix---doc5---1.0\n",
      "sangl---doc5---1.0\n",
      "assassin---doc5---1.0\n",
      "interd---doc5---1.0\n",
      "miet---doc5---1.0\n",
      "tous---doc5---2.0\n",
      "roul---doc5---1.0\n",
      "dit---doc5---1.0\n",
      "appel---doc5---1.0\n",
      "nom---doc5---2.0\n",
      "heur---doc5---1.0\n",
      "temp---doc5---1.0\n",
      "être---doc5---3.0\n",
      "tiroir---doc5---1.0\n",
      "mond---doc5---1.0\n",
      "bleu---doc5---1.0\n",
      "port---doc5---1.0\n",
      "autr---doc5---1.0\n",
      "vacanc---doc5---1.0\n",
      "oui---doc5---1.0\n",
      "ros---doc5---1.0\n",
      "amnes---doc5---1.0\n",
      "fond---doc5---2.0\n",
      "escali---doc5---1.0\n",
      "ça---doc5---2.0\n",
      "quoi---doc5---2.0\n",
      "enfin---doc5---1.0\n",
      "pet---doc5---2.0\n",
      "fait---doc5---1.0\n",
      "puis---doc5---2.0\n",
      "renvers---doc5---1.0\n",
      "mainten---doc5---3.0\n",
      "puisqu---doc5---1.0\n",
      "pourquoi---doc5---1.0\n",
      "dedan---doc5---1.0\n",
      "sanglot---doc5---1.0\n",
      "femm---doc5---1.0\n",
      "fragil---doc5---1.0\n",
      "vingt---doc5---1.0\n",
      "bretel---doc5---1.0\n",
      "tomb---doc5---1.0\n",
      "peur---doc5---1.0\n",
      "saurien---doc5---2.0\n",
      "judici---doc5---1.0\n",
      "bon---doc5---1.0\n",
      "pani---doc5---1.0\n",
      "cel---doc5---1.0\n",
      "fair---doc5---2.0\n",
      "bas---doc5---1.0\n",
      "mémoir---doc5---4.0\n",
      "vert---doc5---1.0\n",
      "mis---doc5---1.0\n",
      "homm---doc5---1.0\n",
      "sauterel---doc5---1.0\n",
      "comment---doc5---1.0\n",
      "vaurien---doc5---1.0\n",
      "têt---doc5---1.0\n",
      "voul---doc5---1.0\n",
      "mot---doc5---4.0\n",
      "histoir---doc5---1.0\n",
      "jour---doc5---1.0\n",
      "rien---doc5---2.0\n",
      "fin---doc5---2.0\n",
      "boul---doc5---3.0\n",
      "crois---doc5---1.0\n",
      "part---doc5---1.0\n",
      "monsieur---doc5---2.0\n",
      "---doc6---3.0\n",
      "veut---doc6---2.0\n",
      "hauss---doc6---1.0\n",
      "ange---doc6---4.0\n",
      "être---doc6---2.0\n",
      "dir---doc6---2.0\n",
      "plus---doc6---1.0\n",
      "cel---doc6---1.0\n",
      "âne---doc6---4.0\n",
      "ail---doc6---1.0\n",
      "si---doc6---1.0\n",
      "tap---doc6---1.0\n",
      "pourt---doc6---1.0\n",
      "chos---doc6---1.0\n",
      "envol---doc6---1.0\n",
      "rien---doc6---1.0\n",
      "étrân---doc6---2.0\n",
      "étrang---doc6---6.0\n",
      "dit---doc6---6.0\n",
      "pied---doc6---1.0\n",
      "quelqu---doc6---1.0\n",
      "---doc7---1.0\n",
      "alor---doc7---1.0\n",
      "froiss---doc7---1.0\n",
      "a---doc7---1.0\n",
      "ogress---doc7---1.0\n",
      "heureux---doc7---1.0\n",
      "valet---doc7---1.0\n",
      "jet---doc7---1.0\n",
      "vont---doc7---1.0\n",
      "effac---doc7---1.0\n",
      "dir---doc7---1.0\n",
      "coeur---doc7---1.0\n",
      "des---doc7---1.0\n",
      "salut---doc7---1.0\n",
      "excus---doc7---1.0\n",
      "plantent---doc7---1.0\n",
      "sal---doc7---1.0\n",
      "cet---doc7---1.0\n",
      "sandwich---doc7---1.0\n",
      "coul---doc7---1.0\n",
      "don---doc7---1.0\n",
      "éboueur---doc7---2.0\n",
      "prospectus---doc7---1.0\n",
      "homm---doc7---1.0\n",
      "tout---doc7---2.0\n",
      "là---doc7---1.0\n",
      "armé---doc7---1.0\n",
      "ru---doc7---3.0\n",
      "tard---doc7---1.0\n",
      "offens---doc7---1.0\n",
      "plais---doc7---1.0\n",
      "cristal---doc7---1.0\n",
      "pardon---doc7---1.0\n",
      "eau---doc7---1.0\n",
      "mécan---doc7---1.0\n",
      "salu---doc7---2.0\n",
      "plein---doc7---2.0\n",
      "pass---doc7---1.0\n",
      "ruisseau---doc7---1.0\n",
      "cont---doc7---1.0\n",
      "chinois---doc7---1.0\n",
      "épé---doc7---1.0\n",
      "grâc---doc7---1.0\n",
      "charm---doc7---1.0\n",
      "plai---doc7---1.0\n",
      "comm---doc7---1.0\n",
      "---doc8---1.0\n",
      "nom---doc8---1.0\n",
      "désir---doc8---1.0\n",
      "sais---doc8---2.0\n",
      "a---doc8---3.0\n",
      "bon---doc8---1.0\n",
      "chanc---doc8---1.0\n",
      "destin---doc8---1.0\n",
      "aim---doc8---1.0\n",
      "mauvais---doc8---1.0\n",
      "grain---doc8---1.0\n",
      "détest---doc8---1.0\n",
      "don---doc8---1.0\n",
      "là---doc8---1.0\n",
      "mépris---doc8---1.0\n",
      "jam---doc8---1.0\n",
      "rien---doc8---1.0\n",
      "appellent---doc8---1.0\n",
      "pourquoi---doc8---1.0\n",
      "bienvenu---doc8---1.0\n",
      "perdu---doc8---1.0\n",
      "appel---doc8---2.0\n",
      "pu---doc8---1.0\n",
      "---doc9---7.0\n",
      "étoil---doc9---2.0\n",
      "revenu---doc9---1.0\n",
      "a---doc9---5.0\n",
      "gard---doc9---1.0\n",
      "dor---doc9---1.0\n",
      "parfum---doc9---1.0\n",
      "fer---doc9---4.0\n",
      "mer---doc9---5.0\n",
      "terr---doc9---6.0\n",
      "tout---doc9---9.0\n",
      "chemin---doc9---4.0\n",
      "grand---doc9---1.0\n",
      "pouss---doc9---2.0\n",
      "tort---doc9---1.0\n",
      "promen---doc9---1.0\n",
      "japon---doc9---1.0\n",
      "derri---doc9---1.0\n",
      "plong---doc9---1.0\n",
      "salu---doc9---1.0\n",
      "soudain---doc9---1.0\n",
      "bien---doc9---1.0\n",
      "voi---doc9---2.0\n",
      "dev---doc9---1.0\n",
      "soleil---doc9---1.0\n",
      "fum---doc9---1.0\n",
      "voil---doc9---2.0\n",
      "marin---doc9---1.0\n",
      "alor---doc9---1.0\n",
      "remerci---doc9---1.0\n",
      "tourn---doc9---1.0\n",
      "arrêt---doc9---1.0\n",
      "manivel---doc9---1.0\n",
      "plus---doc9---1.0\n",
      "bateau---doc9---2.0\n",
      "barri---doc9---1.0\n",
      "traver---doc9---1.0\n",
      "reven---doc9---1.0\n",
      "rencontr---doc9---4.0\n",
      "doigt---doc9---1.0\n",
      "sort---doc9---1.0\n",
      "tous---doc9---1.0\n",
      "roul---doc9---2.0\n",
      "avanc---doc9---1.0\n",
      "emmen---doc9---1.0\n",
      "attrap---doc9---1.0\n",
      "lun---doc9---2.0\n",
      "voitur---doc9---1.0\n",
      "dessus---doc9---1.0\n",
      "fui---doc9---4.0\n",
      "oursin---doc9---1.0\n",
      "cinq---doc9---1.0\n",
      "fond---doc9---1.0\n",
      "écol---doc9---1.0\n",
      "wagon---doc9---1.0\n",
      "pet---doc9---1.0\n",
      "puis---doc9---1.0\n",
      "printemp---doc9---1.0\n",
      "maison---doc9---2.0\n",
      "île---doc9---1.0\n",
      "écras---doc9---1.0\n",
      "pied---doc9---4.0\n",
      "fleur---doc9---1.0\n",
      "main---doc9---1.0\n",
      "naufrag---doc9---1.0\n",
      "peur---doc9---1.0\n",
      "hiv---doc9---2.0\n",
      "beau---doc9---1.0\n",
      "autour---doc9---7.0\n",
      "saumon---doc9---1.0\n",
      "mis---doc9---2.0\n",
      "voul---doc9---2.0\n",
      "coquillag---doc9---1.0\n",
      "sous---doc9---1.0\n",
      "abîm---doc9---1.0\n",
      "mousquetair---doc9---1.0\n",
      "cherch---doc9---1.0\n",
      "cheval---doc9---1.0\n",
      "part---doc9---1.0\n",
      "trois---doc9---1.0\n",
      "---doc10---1.0\n",
      "sais---doc10---4.0\n",
      "être---doc10---3.0\n",
      "a---doc10---2.0\n",
      "musicien---doc10---1.0\n",
      "plus---doc10---1.0\n",
      "blessur---doc10---1.0\n",
      "bless---doc10---3.0\n",
      "archer---doc10---1.0\n",
      "soir---doc10---1.0\n",
      "trop---doc10---2.0\n",
      "long---doc10---1.0\n",
      "coeur---doc10---1.0\n",
      "danger---doc10---1.0\n",
      "visag---doc10---1.0\n",
      "flech---doc10---1.0\n",
      "apres---doc10---1.0\n",
      "amour---doc10---2.0\n",
      "tout---doc10---2.0\n",
      "toujour---doc10---1.0\n",
      "brûl---doc10---2.0\n",
      "peut---doc10---3.0\n",
      "jour---doc10---1.0\n",
      "rien---doc10---1.0\n",
      "harp---doc10---1.0\n",
      "bien---doc10---1.0\n",
      "tendr---doc10---1.0\n",
      "apparu---doc10---1.0\n",
      "chanson---doc10---1.0\n",
      "arc---doc10---1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fichier_inverse():\n",
    "    rExp=r\"doc[0-9]+\"\n",
    "    with open('fichier-inverse.txt', 'w') as f:\n",
    "        dg=dictionnaire_global()\n",
    "        for fich in dg.keys():\n",
    "            for stem in dg[fich].keys():\n",
    "                match=re.search(rExp,fich)\n",
    "                ligne=stem+'---'+match.group(0)+'---'+str(poids(stem,fich))+'\\n'\n",
    "                f.write(ligne)\n",
    "    with open('fichier-inverse.txt','r') as file:\n",
    "        content=file.read()\n",
    "        print(content)\n",
    "fichier_inverse()\n",
    "def stemming_requete(requete):\n",
    "  req=requete.lower()\n",
    "  tokens=re.split(r\"\\W\",req)\n",
    "  mots=[t for t in tokens if not(re.match(r\"([0-9]+-?)+\",t)) and t not in stop_list]\n",
    "  stemmer=SnowballStemmer(\"french\")\n",
    "  stems=[stemmer.stem(mot) for mot in mots]\n",
    "  return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_requete(requete):\n",
    "    dictionnaire={}\n",
    "    req=stemming_requete(requete)\n",
    "    for item in set(req):\n",
    "        dictionnaire[item]=req.count(item)\n",
    "    return dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les 10 documents les plus pertinants selon la requete , metrique de cosinus\n",
      "[('doc3', 0.06401843996644799)]\n",
      "[('doc1', 0.037582301400141446)]\n",
      "[('doc9', 0.02535100632816969)]\n",
      "[('doc2', 0.0)]\n",
      "[('doc4', 0.0)]\n",
      "[('doc5', 0.0)]\n",
      "[('doc6', 0.0)]\n",
      "[('doc7', 0.0)]\n",
      "[('doc8', 0.0)]\n",
      "[('doc10', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "def RSV(doc,requete):\n",
    "  import math\n",
    "  req=stemming_requete(requete)\n",
    "  dict_p={}\n",
    "  nominateur=0\n",
    "  #extraire les mots d'un document a partir de son num et leurs poids\n",
    "  with open('fichier-inverse.txt','r') as file:\n",
    "        content=file.read()\n",
    "        lines = content.splitlines()\n",
    "        for line in lines:\n",
    "          if re.match(r\"^[a-z]+\"+\"---\"+\"doc\"+str(doc)+r\"---[0-9].[0-9]\",line):\n",
    "            mot=re.search(r\"[a-z]+\",line).group()\n",
    "            dict_p[mot]=re.search(r\"[0-9]+.[0-9]\",line).group()\n",
    "            #print(dict_p)\n",
    "  #calcul de la regle de nominateur : poids du terme dans le document\n",
    "  #* poids du meme terme dans la requete\n",
    "  for term in req:\n",
    "    if term in dict_p.keys():\n",
    "      nominateur+=float(dict_p[term])*1\n",
    "  sqrt_p_doc=0\n",
    "  sum_p_doc=0\n",
    "  #calcul de la somme des poids du doc\n",
    "  for poids in dict_p.values():\n",
    "    sum_p_doc+=float(poids)**2\n",
    "  ##print(sum_p_doc)\n",
    "  sqrt_p_doc=math.sqrt(sum_p_doc)\n",
    "  #calcul de la somme des poids de la requete\n",
    "  sum_p_requete= sum(map(lambda x: x**2, TF_requete(requete).values()))\n",
    "  sqrt_p_requete=math.sqrt(sum_p_requete)\n",
    "  RSV=(nominateur)/((sqrt_p_requete)*(sqrt_p_doc))\n",
    "  return [(\"doc\"+str(doc),RSV)]\n",
    "RSV(1,\"colon si\")\n",
    "\n",
    "def RSV_corpus_cosinus(requete,indice):\n",
    "  RSV_corpus=[RSV(doc,requete) for doc in range(1,11)]\n",
    "  sorted_RSV = sorted(RSV_corpus, key=lambda x: x[0][1], reverse=True)\n",
    "  print(\"les \"+str(indice) +\" documents les plus pertinants selon la requete , metrique de cosinus\")\n",
    "  for i in range(0,indice):\n",
    "    print(sorted_RSV[i])\n",
    "RSV_corpus_cosinus(\" la couleure du soleil est blue\",10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
